{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPBmpjOuLQLW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Check device configurations\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create MNIST and CIFAR transform objects\n",
        "mnist_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)])\n",
        "cifar_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Define hyper-parameters\n",
        "batch_size = 4\n",
        "mnist_input_size = 1\n",
        "mnist_dimensions = 28\n",
        "cifar_input_size = 3\n",
        "cifar_dimensions = 32\n",
        "num_epochs = 20\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Establish the MNIST data set and loaders\n",
        "mnist_trainset = torchvision.datasets.MNIST(root='./mnist_data',\n",
        "                                            transform=mnist_transform,\n",
        "                                            train=True,\n",
        "                                            download=True)\n",
        "mnist_testset = torchvision.datasets.MNIST(root='./mnist_data',\n",
        "                                           transform=mnist_transform,\n",
        "                                           train=False,\n",
        "                                           download=True)\n",
        "mnist_trainloader = torch.utils.data.DataLoader(dataset=mnist_trainset,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=2)\n",
        "mnist_testloader = torch.utils.data.DataLoader(dataset=mnist_testset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=False,\n",
        "                                               num_workers=2)\n",
        "\n",
        "# Establish the CIFAR datasets and loaders\n",
        "cifar_trainset = torchvision.datasets.CIFAR10(root='./cifar_data',\n",
        "                                              transform=cifar_transform,\n",
        "                                              train=True,\n",
        "                                              download=True)\n",
        "cifar_testset = torchvision.datasets.CIFAR10(root='./cifar_data',\n",
        "                                             transform=cifar_transform,\n",
        "                                             train=False,\n",
        "                                             download=True)\n",
        "cifar_trainloader = torch.utils.data.DataLoader(cifar_trainset,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=2)\n",
        "cifar_testloader = torch.utils.data.DataLoader(cifar_testset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=False,\n",
        "                                               num_workers=2)\n",
        "\n",
        "# Convoluted neural networks\n",
        "class NeuralNet1(nn.Module):\n",
        "    def __init__(self, input_size, dimensions):\n",
        "        super(NeuralNet1, self).__init__()\n",
        "        self.conv = nn.Conv2d(input_size, 16, 5, padding='same')\n",
        "        self.fulc = nn.Linear(16*dimensions**2, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fulc(x)\n",
        "        return x\n",
        "class NeuralNet2(nn.Module):\n",
        "    def __init__(self, input_size, dimensions):\n",
        "        super(NeuralNet2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, 6, 5, padding='same')\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, padding='same')\n",
        "        self.fulc1 = nn.Linear(16*dimensions**2, 84)\n",
        "        self.fulc2 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.relu(self.fulc1(x))\n",
        "        x = self.fulc2(x)\n",
        "        return x\n",
        "class NeuralNet3(nn.Module):\n",
        "    def __init__(self, input_size, dimensions):\n",
        "        super(NeuralNet3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, 6, 5, padding='same')\n",
        "        self.norm1 = nn.BatchNorm2d(6)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, padding='same')\n",
        "        self.norm2 = nn.BatchNorm2d(16)\n",
        "        self.fulc1 = nn.Linear(16*(dimensions//4)**2, 120)\n",
        "        self.fulc2 = nn.Linear(120, 84)\n",
        "        self.fulc3 = nn.Linear(84, 10)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.norm1(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.norm2(self.conv2(x))))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.relu(self.fulc1(x))\n",
        "        x = self.relu(self.fulc2(x))\n",
        "        x = self.fulc3(x)\n",
        "        return x\n",
        "\n",
        "# Create network models\n",
        "mnistnet1 = NeuralNet1(mnist_input_size,mnist_dimensions).to(device)\n",
        "mnistnet2 = NeuralNet2(mnist_input_size,mnist_dimensions).to(device)\n",
        "mnistnet3 = NeuralNet3(mnist_input_size,mnist_dimensions).to(device)\n",
        "mnistnets = [mnistnet1, mnistnet2, mnistnet3]\n",
        "cifarnet1 = NeuralNet1(cifar_input_size,cifar_dimensions).to(device)\n",
        "cifarnet2 = NeuralNet2(cifar_input_size,cifar_dimensions).to(device)\n",
        "cifarnet3 = NeuralNet3(cifar_input_size,cifar_dimensions).to(device)\n",
        "cifarnets = [cifarnet1, cifarnet2, cifarnet3]\n",
        "\n",
        "# Train and test each MNIST net first\n",
        "print('Initiating MNIST Training for', len(mnistnets), 'networks')\n",
        "for mnistnet in mnistnets:\n",
        "    # Initiate timer, TensorBoard writer, loss criterion, and optimizer\n",
        "    time_start = time.time()\n",
        "    writer = SummaryWriter()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(mnistnet.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train model by looping over the dataset per epoch per data\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(mnist_trainloader, 0):\n",
        "            # Extract data tuple to device in inputs and labels, respectively\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Zero out parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass then backpropagate and optimize\n",
        "            outputs = mnistnet(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Print progress for every databatch of 2500\n",
        "            running_loss += loss.item()\n",
        "            if i % 2500 == 2499:\n",
        "                print(f'[{epoch + 1}/{num_epochs}, {i + 1:5d}/{len(mnist_trainloader)}] loss: {running_loss / 2500:.3f}')\n",
        "                writer.add_scalar(\"Loss/train\", running_loss, epoch * len(mnist_trainloader) + i)\n",
        "                running_loss = 0.0\n",
        "    # Calculate time it took to train\n",
        "    time_total = (time.time() - time_start)\n",
        "    print('Finished MNIST Training', str(mnistnets.index(mnistnet)+1) + '/' + str(len(mnistnets)), 'in', str(int(time_total//60)) + 'm' + str(int(time_total%60)) + 's')\n",
        "\n",
        "    # Model testing (gradients are not computed for memory efficiency)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in mnist_testloader:\n",
        "            # Parse data to our device, whether NVIDIA CUDA or the CPU\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Run the images through the network to get our outputs\n",
        "            outputs = mnistnet(images)\n",
        "            # Predict the class with the highest energy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Adjust prediction counters and print the given accuracy\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print('Accuracy of the network on the', len(mnist_testloader), 'test images: {} %'.format(100 * correct / total))\n",
        "    print('Finished MNIST Testing', str(mnistnets.index(mnistnet)+1) + '/' + str(len(mnistnets)) + '/n')\n",
        "\n",
        "    # Flush and close TensorBoard writer then save the model checkpoint\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "    torch.save(mnistnet.state_dict(), 'mnistnet' + str(mnistnets.index(mnistnet)+1) + '.ckpt')\n",
        "\n",
        "# Repeat train and test with CIFAR nets (retains same comments for clarity)\n",
        "print('\\n\\nInitiating CIFAR Training for', len(cifarnets), 'networks')\n",
        "for cifarnet in cifarnets:\n",
        "    # Initiate timer, TensorBoard writer, loss criterion, and optimizer\n",
        "    time_start = time.time()\n",
        "    writer = SummaryWriter()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(cifarnet.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train model by looping over the dataset per epoch per data\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(cifar_trainloader, 0):\n",
        "            # Extract data tuple to device in inputs and labels, respectively\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Zero out parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass then backpropagate and optimize\n",
        "            outputs = cifarnet(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Print progress for every databatch of 2500\n",
        "            running_loss += loss.item()\n",
        "            if i % 2500 == 2499:\n",
        "                print(f'[{epoch + 1}/{num_epochs}, {i + 1:5d}/{len(cifar_trainloader)}] loss: {running_loss / 2500:.3f}')\n",
        "                writer.add_scalar(\"Loss/train\", running_loss, epoch * len(cifar_trainloader) + i)\n",
        "                running_loss = 0.0\n",
        "    # Calculate time it took to train\n",
        "    time_total = (time.time() - time_start)\n",
        "    print('Finished CIFAR Training', str(cifarnets.index(cifarnet)+1) + '/' + str(len(cifarnets)), 'in', str(int(time_total//60)) + 'm' + str(int(time_total%60)) + 's')\n",
        "\n",
        "    # Model testing (gradients are not computed for memory efficiency)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in cifar_testloader:\n",
        "            # Parse data to our device, whether NVIDIA CUDA or the CPU\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Run the images through the network to get our outputs\n",
        "            outputs = cifarnet(images)\n",
        "            # Predict the class with the highest energy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Adjust prediction counters and print the given accuracy\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print('Accuracy of the network on the', len(cifar_testloader), 'test images: {} %'.format(100 * correct / total))\n",
        "    print('Finished CIFAR Testing', str(cifarnets.index(cifarnet)+1) + '/' + str(len(cifarnets)) + '/n')\n",
        "\n",
        "    # Flush and close TensorBoard writer then save the model checkpoint\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "    torch.save(cifarnet.state_dict(), 'cifarnet' + str(cifarnets.index(cifarnet)+1) + '.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of the models that had the least amount of error for validation, the MNIST models were far more accurate than the CIFAR models were. The MNIST models were highly precise, with a range of 0.67. They also had near-perfect accuracy, that only improved with each succesive network. Conversely, the CIFAR models were less optimal, with a wider accuracy range of 9.96. Additionaly, their accuracy was 63.21% at best. It should be noted that their accuracy did increase with each successive model. This is especially the case with the last neural network, as it used batch normalization. It should be noted that the CIFAR models had computed faster than the MNIST models.\n",
        "\n",
        "MNIST TIME AND ACCURACY\n",
        "*   Net 1 - 18m45s (98.52%)\n",
        "*   Net 2 - 22m09s (99.01%)\n",
        "*   Net 3 - 25m53s (99.19%)\n",
        "\n",
        "CIFAR TIME AND ACCURACY\n",
        "*   Net 1 - 15m49s (53.25%)\n",
        "*   Net 2 - 18m27s (55.37%)\n",
        "*   Net 3 - 21m58s (63.21%)"
      ],
      "metadata": {
        "id": "KslLLjSpDDjj"
      }
    }
  ]
}